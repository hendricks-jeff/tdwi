{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Describing Data Using Pandas\n",
    "## Notebook Outline:\n",
    "\n",
    "* <a href='#IntroToPandas'>Introduction To Pandas</a>\n",
    "* <a href='#LoadingStandardCSV'>Loading Standard CSV</a>\n",
    "* <a href='#BasicDataDescription'>Basic Data Description</a>\n",
    "* <a href='#LoadingTabDataFile'>Loading Tab DataFile</a>\n",
    "* <a href='#LoadingWhiteSpaceFile'>Loading White Space File</a>\n",
    "* <a href='#WritingOutToCSV'>Writing Data Out To A CSV</a>\n",
    "* <a href='#LessonSummary'>Lesson Summary</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use this Notebook\n",
    "\n",
    "The best way to use this notebook is to follow along with the lecture and then to apply what you learn to your own data files, or (if you do not have any of your own data) to practice using this functions and methods on the provided data. A little practice goes a long way towards understand and retaining! It would be easy to just skim this notebook, but you will learn more by doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Introduction To Pandas - What is Pandas?\n",
    "### From the Pandas website:\n",
    "http://pandas.pydata.org/\n",
    "\n",
    "'pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.'\n",
    "\n",
    "If you are familiar with Excel, think of Pandas as a similar tool to explore and analyze data. There are big differences between Pandas and Excel (Pandas is faster, can handle larger datasets more efficiently, and can do more overall, but does not have GUI), but they can be used for similar purposes and having that comparison in your mind may help you digest the information.\n",
    "\n",
    "### My experience with Pandas:\n",
    "\n",
    "I use Pandas everyday, along with Jupyter Notebook, to explore and analyze client data. It is an integral part of my real-world-workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='LoadingStandardCSV'></a>\n",
    "# Loading A Standard CSV File With Pandas\n",
    "In the below cells, we import pandas. Then, we load a file of the most popular baby boy names used in Illinois from 1980 to 2013. Please see the comments in each cell below for more details about the code in each cell. Also - please see the lecture videos that walk through this notebook!\n",
    "\n",
    "Also, we will be using the `read_csv()` method extensively, and introducing some of its arguments.  If you'd like, you can refer to its documentation here <https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we must import pandas.  It is very common to import pandas as pd.  All\n",
    "# this means is that I can refer to pandas as 'pd' in my code - saving myself\n",
    "# from typing 4 more characters and also saving space.\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we need to define the filepath to our file.  We will go over this in \n",
    "# the lecture. Also note that I wrap the path in parentheses, this allows me\n",
    "# to write the string on multiple lines. This just keeps my code tidy. I will\n",
    "# explain this more in the video.\n",
    "\n",
    "\n",
    "filepath = os.path.join(os.getcwd(), 'data', 'Most_Popular_Baby_Boy_Names__1980-2013.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load our data.  It is pretty simple, we just use the read_csv()\n",
    "# method. Method docs can be found here:\n",
    "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "\n",
    "print(filepath)\n",
    "nameData = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the type of the object we just created\n",
    "print(type(nameData))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='BasicDataDescription'></a>\n",
    "# Basic Data Description\n",
    "\n",
    "Now we start our basic data description!  Unfortunately, \"basic\" can sometimes sound like something is not interesting, or not \"the good stuff\" - this is definitely not true in this case. It is realtively simple, but it is very important to have a solid high-level understanding of your data before you dive in deeper. If you skip this, you will end up paying for it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The .head() method can be used to get the first n lines of a dataframe. It is always a good idea to just 'look' at your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below we print the first 3 lines of the data file. The default number of\n",
    "# lines printed is 5\n",
    "nameData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The .tail() method can be used to get the last n lines of a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameData.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just learned something by looking at the data - it looks like the names were entered in all caps in some of the data. This will be important later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The .sample() method can be used to get a random smaple of n rows from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameData.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The .shape _attribute_ will tell us the size of the file; the number of rows and the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The .columns attribute will tell us the names of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameData.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The .dtypes attribute will display the variable type of each column.\n",
    "* This can be helpful in detemrining what the contents of each column is (see the auto mpg example below).\n",
    "* 'object' is used for strings or other variable types thare not numbers or dates.  For example, lists or tuples, which can be stored in a dataframe, but that is rare - most of the time, when you see 'object' it means the column contains strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameData.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The .info() method will also tell is the datatype, but with some additional info about the size of the dataframe and the number of non-null entries. \n",
    "A null entry would be one that is _empty_ in the dataset.  Remember that sometimes the dataset already comes with null or missing values marked with a special value, like -9999 (we will see this in the weather data example). Pandas will not immediately recognize this as a null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The .memory_usage() method gives the size of each column in bytes.\n",
    "Note that if you add these together and divide by 1024 (1024 bytes = 1 KB), you get the same number that is shown in the output from .info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "nameData.memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The .describe() method outputs basic descriptive statistics about all of the _numerical_ columns in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameData.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The .unique() method will output the unique values in a column.\n",
    "In order to get a column from a dataframe, simple put the column name in square brackets after the dataframe variable. For example, we use nameData['Name'] below to get the name column of the dataframe. (We will cover indexing and slicing of dataframes in greater detail in a following lesson.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameData['Name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The .nunique() method will output the number of uniuqe values in a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameData['Name'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The .value_counts() method will output the number of times each value occurs in a column. \n",
    "For example, we see that \"Christoper\" has been ranked 26 times, and 'CHRISTOPHER' 5 times.  So, in actuallity, the name has been ranked 31 times. We will come back to this in a future lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameData['Name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief summary of what you have learned so far:\n",
    "* head(n) - get the first n rows\n",
    "* tail(n) - get the last n rows\n",
    "* sample(n) - get a random sample of n rows\n",
    "* shape - get the number of rows and columns\n",
    "* columns - get the column names\n",
    "* dtypes - get the variable types of each column\n",
    "* info() - get the variables types, non-null counts, and memory size of the DataFrame\n",
    "* memory_usage() - get the memory usage of each column of the data frame\n",
    "* describe() - get basic summary statistics about each numerical column\n",
    "* unique() - get the unique values in a column\n",
    "* nunique() - get the number of unique values in a column\n",
    "* value_counts() - get the occurence counts for each value in a column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='LoadingTabDataFile'></a>\n",
    "# Loading A Data File With Tab Separated Fields\n",
    "Great work so far! You have learned how to use pandas to get a high-level description of your dataset.  We are now going to apply these same functions to another dataset, and also learn some new functionality (that I use often) in the process.\n",
    "\n",
    "The next dataset we are going to load is a dataset of car models made from 1970 to 1982. The dataset includes the following attributes of each model: The mpg, number of cylinders, engine displacement, horsepower, weight, acceleration (m/s^2), model year and car name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducing the 'sep' argument in the read_csv() method.\n",
    "The sep argument allows us to specify the field separator that pandas should use when attempting to read in the data. Below, we set it to the tab escape sequence which is '\\t'. (This just means that '\\t' indicates a tab). Note that the default value for the 'sep' argument is ',' which is why we do not have to set it when reading in comma separated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(os.getcwd(), 'data', 'auto-mpg-tabs.csv')\n",
    "\n",
    "autoMPGData = pd.read_csv(filepath, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the .head() method to look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoMPGData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducing the index_col argument to the read_csv() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the first column 'Unnamed: 0'. The reason we see this in the dataframe is because this file already has an index column (see the screenshot below).  Pandas always automatically adds its own index column. So, it treats the index column in the file as a column of data. Since this column has no header in the file, it gives it a generic heading of 'Unnamed: 0'. We can use the 'index_col' argument when reading in a csv to indicate which column, already present in the datafile, we would like to use as the index.  In this case, we want to use the first column. Remember that Python is zero-indexed, so the first column will be column 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note how we use the index_col argument to read in the first column, in the data file, as the index.\n",
    "autoMPGData = pd.read_csv(filepath, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice below that we no longer have the extra 'Unnamed: 0' column when we use .head() below to get the first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoMPGData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .shape, .info() and describe() to better understand the data set.\n",
    "Notice below how the horsepower data type is 'object' and not 'int64' or 'float64'.  Horsepower is a number, so we would expect the datatype to be an int or float.  But pandas as recognized it as 'object' (which means that pandas has recognized the column as a column of strings).  This is unexpected, and means that there probably is a string in the data! We will see what it is using some of the other methods we have learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape\n",
    "autoMPGData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info()\n",
    "autoMPGData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe()\n",
    "autoMPGData.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking a closer look at the hourspower column using .unique()\n",
    "This is not the only way to find the bad value. But this is one way, using a method we have learned so far. We will see some other possibilities in coming lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique()\n",
    "autoMPGData['horsepower'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory_usage()\n",
    "autoMPGData.memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='LoadingWhiteSpaceFile'></a>\n",
    "# Loading A Data File With Fields Delimited By White Space\n",
    "We will now look at one more data file. This file is from the isd-lite data that can be found here: <ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-lite>\n",
    "\n",
    "These files contain weather observations from weather stations all over the world.  We will look at the 2001 data for the station 724080-13739 which is a station at the Philadelphia International Airport.\n",
    "\n",
    "This particular data is delimited by white space. White space can mean a number of things: tabs, spaces, new lines.  In this case it just means spaces; see the screen shot below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducing the 'delim_whitespace' argument\n",
    "We can use a special argument when a datafile is separated by an undetermined amount of white space. That is, field could be separated by different number of spaces, or tabs and spaces etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(os.getcwd(), 'data', 'Philadelphia_Pennsylvania_USA/724080-13739-2001')\n",
    "weatherData = pd.read_csv(filepath, delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to set column names for datafiles without column names.\n",
    "Notice in the above cell, we see that pandas reads the first line as the column names. But, in this file, there are no names and the first line is data. There are two different strategies to solve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to set column names by using the 'names' argument when we read in the data.\n",
    "If we know what the column names should be, we can pass them to the names argument as a list, and pandas will automatically apply the names to the columns when it reads in the data.\n",
    "\n",
    "We know what the column names should be, by looking at the data documentation which is here: <ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-lite/isd-lite-format.pdf>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['Year', 'Month', 'Day', 'Hour', 'Air Temp', 'Dew Point Temp',\n",
    "           'Sea Level Pressure',\n",
    "           'Wind Direction', 'Wind Speed Rate',\n",
    "           'Sky Condition Total Coverage Code',\n",
    "           'Liquid Precipitation Depth Dimension - 1Hr Duration',\n",
    "           'Liquid Precipitation Depth Dimension - Six Hour Duration']\n",
    "print(headers)\n",
    "weatherData = pd.read_csv(filepath, delim_whitespace=True,\n",
    "                          names=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherData.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the 'header' argument and setting the columns after we read in the file.\n",
    "Another method is to use the 'header' argument to prevent pandas from reading in (and applying) any column names and then setting the columns names with the column attribute. See below how we set the header argument to None. The default value is 0, which means that pandas will try to read the first row as the header of the data file (the column names).  Remember that python is zero-indexed, so a value of 0 indicates the first row. By setting header to None we are \"telling\" .read_csv() that it should not treat any row as the headers when reading the file, and it will just number the columns 0 through 11.\n",
    "\n",
    "When then set the columns attribute to be the list of column names the we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherData = pd.read_csv(filepath, delim_whitespace=True, header=None)\n",
    "weatherData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherData.columns = headers\n",
    "weatherData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherData.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the -9999 values?\n",
    "You have probably noticed the -9999 values in the 'Liquid Precipitation Depth Dimension - Six Hour Duration' column.  Without knowing anything more, we should be very suspicious that this is a special value indicating a missing value.  If we look at the data documentation linked in a previous cell, we will see that -9999 is used as a missing value.  We will come back to missing values in a future lecture, and we will specifically look at this example.  For now, we note it and move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .shape, .dtypes, .info(), and .describe() to take a closer look at the weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherData.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherData.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigating The Sky Condition Total Coverage Code Using value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 - No Clouds\n",
    "# 2 - 2 Oktas\n",
    "# 4 - 4 Oktas\n",
    "# 6 - 6 Oktas\n",
    "# 7 - 7 Oktas\n",
    "# 8 - 8 Oktas\n",
    "# 9 - Sky obscured or cloud amount can not be estimated\n",
    "# -9999 - Missing\n",
    "weatherData['Sky Condition Total Coverage Code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherData['Year']+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we divided by the total rows and multipled by 100 to get the % of each\n",
    "# cloud cover type in the data.\n",
    "(weatherData['Sky Condition Total Coverage Code'].value_counts() / weatherData.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that many of the methods and attributes we have used return DataFrames or Series as output. \n",
    "A series is like a dataframe except that is one dimensional.\n",
    "Note below how I show that the types of the returned output from the .value_counts() and .describe() methods are dataframes and series.  This is true for many of operations we will apply to dataframes. Just something to keep in mind as we continue to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='WritingOutToCSV'></a>\n",
    "# How To Write Data Back Out To A CSV\n",
    "\n",
    "To write data out to a csv, we can use the `to_csv()` method. The doc are here: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html\n",
    "\n",
    "Below, let's use `to_csv()` to write out our weatherData dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create the path to the file we would like to create\n",
    "save_path = os.path.join(os.getcwd(), 'data', 'Philadelphia_Pennsylvania_USA/724080-13739-2001_out')\n",
    "\n",
    "# Then use to_csv\n",
    "weatherData.to_csv(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's read it back in and look at the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherData2 = pd.read_csv(save_path)\n",
    "weatherData2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice there is now another column - why?\n",
    "\n",
    "The reason there is another column, is that when we use `to_csv()`, pandas writes out the index to the file. But, when we read in the file it will treat that column of a data as a column of data and not an index. So, we need to either use the `index_col` argument with `read_csv()` to read in the column as an index (like we saw above), or we can set the `index` argument to False when we use `to_csv()`.\n",
    "\n",
    "Let's see this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherData.to_csv(save_path, index=False)\n",
    "weatherData2 = pd.read_csv(save_path)\n",
    "weatherData2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='LessonSummary'></a>\n",
    "# Lesson Summary:\n",
    "In this lesson you learned about the following:\n",
    "* Methods and attributes that help describe data files:\n",
    "    * head(n) - get the first n rows\n",
    "    * tail(n) - get the last n rows\n",
    "    * sample(n) - get a random sample of n rows\n",
    "    * shape - get the number of rows and columns\n",
    "    * columns - get the column names\n",
    "    * dtypes - get the variable types of each column\n",
    "    * info() - get the variables types, non-null counts, and memory size of the dataframe\n",
    "    * memory_usage() - get the memory usage of each column of the data frame\n",
    "    * describe() - get basic summary statistics about each numerical column\n",
    "    * unique() - get the unique values in a column\n",
    "    * nunique() - get the number of unique values in a column\n",
    "    * value_counts() - get the occurrence counts for each value in a column\n",
    "<br>\n",
    "<br>\n",
    "* Arguments to the read_csv() method that help you read in various file types:\n",
    "    * sep - an argument that allows you to specify the field separate used (we saw commas and tabs)\n",
    "    * index_col - an argument to specify the column used as the index\n",
    "    * names - an argument to specify column names\n",
    "    * header - an argument to specify which row to use as the header\n",
    "    * columns - an attribute that can be set, to change the column names of a dataframe\n",
    "<br>\n",
    "<br>\n",
    "* How to use to_csv() to write out data and how to use the index argument.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Class Exercise\n",
    "In the cells below, Load the file \"AAA_Fuel_Prices.csv\" and use some of the methods we learned above to explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(os.getcwd(), 'data', 'AAA_Fuel_Prices.csv')\n",
    "\n",
    "aaaFuel=pd.read_csv(filepath)\n",
    "\n",
    "aaaFuel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaaFuel.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaaFuel.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaaFuel['Fuel'].unique()\n",
    "aaaFuel['Fuel'].nunique()\n",
    "aaaFuel['Fuel'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question or Comments About This Notebook?\n",
    "Feel free to contact me via my LinkedIn: https://www.linkedin.com/in/william-j-henry <br>\n",
    "You can also email me at will@henryanalytics.com <br>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
